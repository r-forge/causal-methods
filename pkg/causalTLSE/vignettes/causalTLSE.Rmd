---
title: "Semiparametric Thresholding Least Squares Inference for Causal Effects with R"
author: "Pierre Chausse^[University of Waterloo, pchausse@uwaterloo.ca], Mihai Giurcanu^[University of Chicago, giurcanu@uchicago.edu], George Luta^[Georgetown University, George.Luta@georgetown.edu]"
date: ""
output: rmarkdown::pdf_document
abstract: "The vignette explains how to use the causalTLSE package to estimate thet causal effects using a semiparametric thresholding least squares methods."
vignette: >
  %\VignetteIndexEntry{Semiparametric Thresholding Least Squares Inference for Causal Effects}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{causalTLSE}
  %\VignettePackage{causalTLSE}
  %\VignetteEncoding{UTF-8}
header-includes:
- \DeclareMathOperator{\Ex}{E}
- \DeclareMathOperator{\var}{var}
- \DeclareMathOperator{\pr}{Pr}
- \newcommand{\tp}[1]{#1^T}
- \newcommand{\reals}{\mathbb{R}}
---

# Introduction

This document presents the `causalTLSE` package explaining in details
all functions implemented in the package. It is intended for users
interested in all the details about the methods presented in the paper
and how they are implemented.

The main model is 

\[ 
Y = \beta_0 (1-Z) + \beta_1 Z + f_0(X) + f_1(X) + \varepsilon\,, 
\] 

and it is approximated by the regression

\[ 
Y = \beta_0 (1-Z) + \beta_1 Z + \tp{\psi_0}U_0(X)(1-Z) + 
\tp{\psi_1}U_1(X)Z + u\,, 
\] 

where $Y\in \reals$ is the response variable, $X$ is a $k\times 1$
vector of confounders, and $U_0(X)\in \reals^{p_0}$ and $U_1(X)\in
\reals^{p_1}$ are spline vectors. We can represent $U_j(X)$, for
$j=0,1$, into a block vector $\tp{\{\tp{U_{j1}(X_1)},
\tp{U_{j2}(X_2)}, ..., \tp{U_{jk}(X_k)}\}}$, where $U_{jl}(X_l)\in
\reals^{p_{jl}}$ is the vector of basis functions for group $j$
associated with $X_l$, with $\sum_{l=1}^k p_{jl}=p_j$, $j=0,1$. The
paper proposes a data-driven method for selecting the vectors $U_0(X)$
and $U_1(X)$.

To understand the package, it is important to know how the
$U_{jl}(X_l)$'s are defined. To simplify the notation, we remove the
subscript $l$ from $X$ and the subscripts $j$ and $l$ from
$U_{jl}(X_l)$ and $p_{jl}$. We just need to keep in mind that
$U(X)=\{U_s(X)\}\in\reals^p$ is different for the treated and control
groups and also for different confounders. Let $\{\kappa_{1}, ...,
\kappa_{p-1}\}$ be a set of $p-1$ knots strictly inside the support of
$X$ satisfying $\kappa_{1}<\kappa_2<,...,< \kappa_{p-1}$. For a
realization $x$ and $p\geq 3$, we have the following bases:

\begin{eqnarray*}
U_1(x) &=& xI(x\leq \kappa_{1}) + \kappa_{1}I(x> \kappa_{1}) \\
U_p(x) &=& (x - \kappa_{p-1})I(x > \kappa_{p-1})\\
U_s(x) &=& (x -  \kappa_{s-1})I(\kappa_{s-1} \leq x \leq \kappa_{s}) +
(\kappa_{s} - \kappa_{s-1})I(x> \kappa_{s})\,,
\end{eqnarray*}

where the last $U_s(x)$ is defined for $2<s<p$. Therefore, if the
number of knots is equal to 1, we only have the first two bases. Since
the knots must be strictly inside the support of $X$, for any
categorical variable with two levels, which includes as a special case
binary variables, the number of knots must be equal to zero. In this
case, $U(X)=X$. For general nominal variables, the number of knots
cannot exceed the number of levels minus two.

Note that for the sample regression

\[ 
Y_i = \beta_0 (1-Z_i) + \beta_1 Z_i + 
\tp{\psi_0}U_0(X_i)(1-Z_i) + \tp{\psi_1}U_1(X_i)Z_i + u_i\,, 
\] 

for $i=1,...,n$, the knots of $X_l$, $l=1,...,k$, must be strictly
inside the sample range of the vector $\{X_{li}\}\in\reals^n$ instead
of inside the support of $X_l$.

# The `causalTLSE` package

## Setting up the Model

The first step in using the package is to define a model. The model
contains the information about the outcome, the treatment indicator,
the covariates and their knots. This is the starting point before
applying any basis selection method. To illustrate how to use the
package, we are using the dataset from Lalonde (1986). The dataset,
called `nsw`, contains some continuous and categorical variables, so
we can illustrate how knots are selected initially. The dataset is
included in the `causalTLSE` package.

```{r}
library(causalTLSE)
data(nsw)
```

The outcome is the real income in 1978 (`re78`) and the purpose is to
estimate the causal effect of a training program (`treat`) on the
outcome. The dataset includes the continuous covariates age (`age`),
education (`ed`) the 1975 real income (`re75`) and some binary
variables (`black`, `hisp`, `married` and `nodeg`). We start by
considering the covariates `age`, `re75`, `ed` and ``married`. We can
setup the model simply by running the following command:

```{r}
model1 <- setModel(re78 ~ treat | ~ age + re75 + ed + married, data=nsw)
```

The left of | is designated for the formula linking the outcome (re78)
and the treatment indicator (treat). The covariates are entered after
| as a formula without a dependent variable. This method works like for formulas in `lm`. For example, we can add interactions, transformations of the variables, etc. The following is an example:

```{r}
model0 <-  setModel(re78 ~ treat | ~ age + I(age^2) + re75 + ed * married,
                   data=nsw)
```

This will create the vector of covariates {age, age$^2$, re75, ed,
married, ed$\times$married}. The function `setModel` creates an object
of class `tlseModel` with its own print method, which will be
presented later. 

The following sub-sections explain all arguments of the function.

### The starting knots

By default, the function automatically generates knots for each
variable based on the following procedure. This procedure is applied
separately for the treated and control groups. The term `sample size`
means the number of observations in the treated or control group.

1. The starting number of knots is a function of the sample size and
   is determined by the argument `nbases`, a function of one argument,
   the sample size. The floor of what the function returns is the
   number of bases. The starting number of knots is therefore equal to
   the `floor` of what the function returns minus 1 (or 0 if this
   operation results in a negative number). The default function is
   `function(n) n^0.3`. For example, if the total sample size is 500,
   with 200 treated and 300 control, the starting number of knots in
   the treated and control groups are respectively equal to 3
   (`floor(200^0.3)-1`) and 4 (`floor(300^0.3)-1`). It is possible to
   have a number of knots that does not depend on the sample size. All
   we need is to set the argument `nbases` to a function that returns
   an integer.
   
2. Let $(p-1)$ be the number of knots determined by the previous
   step. The knots are obtained by computing $p+1$ quantiles of $X$
   for equally spaced probabilities from 0 to 1, and by dropping the
   first and last ones. For example, if the number of knots is equal
   to 3, we compute the quantiles for the probabilities
   0.25, 0.5 and 0.75.
   
3. We drop any duplicated knots and any knots equal to either the max
   or the min of X. If the resulting number of knots is equal to 0,
   the vector of knots is set to `NULL`. When the knots is `NULL` for
   a variable $X$, it means that $U(X)=X$.
   
The last step implies that the number of knots for all categorical
variables with two levels, which includes as a special case binary
variables, is equal to 0. For nominal variables with a small number of
levels, the number of knots may be smaller than the ones defined by
`nbases`. For example, when the number of levels for a nominal
variable is 3, the number of knots cannot exceed 1.

We can inspect the knots of the current model as follows. Note that
each object in the package is S3-class, so the elements can be
accessed using the operator $. The elements `knots0` and `knots1` are
the list of knots for the control and treated groups. For example, the
knots for the treated are:

```{r}
model1$knots1
```

We see that it is set to `NULL` for `married`, because it is a binary
variable. The sample size for the treated is `r sum(nsw$treat==1)`.
Given the default `nbases`, it implies a number of starting knots
equal to `r floor(sum(nsw$treat==1)^.3)-1` ($297^{0.3}-1=4.519$). This
is the number of knots we have for `ed` and `age`, but not for
`re75`. The reason is that `re75` contains a large fraction of
zeros. Since the 20\% quantile is equal to 0 and 0 in also the minimum
value of `ed75`, it is dropped (the `type` argument of the `quantile`
function is the same as it is implemented in the package).

```{r}
quantile(nsw[nsw$treat==1,'re75'], c(.2,.4,.6,.8), type=1)
```
   
By printing the object, we see a summary of the model. It includes the
list of variable with a positive number of knots and the ones with no
knots. 

```{r}
model1
```

> **SLSE**: We see that the selection method is set to SLSE, which
> stands for Semiparametric Least Squares Estimator. We refer to this
> when the knots are automatically selected by the method described
> above. Later in the document, we will present methods for selecting
> a subset of the SLSE using TLSE.

As another example, the simulated dataset `simDat4` contains special
types of covariates. It helps to further illustrate how the knots are
determined. The dataset contains a continuous variable `X1` with a
large proportion of zeros, the nominal variables `X2` and `X3`, with
respectively 2 and 3 levels (the levels are {3,4} for `X2` and {1,2,3}
for `X3`), and a binary variable `X4`.

```{r}
data(simDat4)
model2 <- setModel(Y~Z |~X1+X2+X3+X4, data=simDat4)
model2$knots0
```

We see that the number of knots for the nominal variable with 2
levels is set to 0 and it is equal to 1 for the one with 3 levels.

### Setting the number of knots to 0 for specific variables

To avoid having a positive number of knots for a variable, we can
enter its name in the argument `userRem`. For example, if we want the
number of knots to be zero for `ed` and `age`, we can create the model
as follows:

```{r}
model3 <- setModel(re78~treat | ~age+re75+ed+married, data=nsw,
                   userRem=c("ed","age"))
model3
```

We see that only `re75` has a positive number of knots. 

### Setting the knots manually

We have the control over the knots through the arguments `knots0` and
`knots1`. When the arguments are missing (the default), all knots are
set automatically. One way to set the number of knots to 0 for all
variables in a given group is to set the argument to `NULL`. For
example, the number of knots is equal to 0 for all variables of the
treated group in the following:

```{r}
setModel(re78~treat | ~age+re75+ed+married, data=nsw, knots1=NULL)
```

Notice that the selection method is defined as "User Based" whenever
knots are provided manually by the user. The other option is to
provide a list of knots. The list must have the same length as the
number of covariates. For each element, we have three options:

- `NA`: The knots are set automatically for this variable only.

- `NULL`: The number of knots is set to 0 for this variable only.

-  A numeric vector: The vector cannot contain missing or duplicated
   values and must be strictly inside the range of the variable for
   the group.
   

Suppose you want to set for the control group an automatic selection
for `age`, no knots for `ed` and the knots $\{1000, 5000, 10000\}$ for
`re75`, and the knots be automatically selected for the treated
group. We proceed as follows. Note that setting the value to `NA` or
`NULL` has the same effect for the binary variable `married`. In the
following, the argument `knots=TRUE` is added to the `print` method to
only print the knots.

```{r}
model <- setModel(re78~treat | ~age+re75+ed+married, data=nsw,
                  knots0=list(NA, c(1000,5000,10000), NULL, NA))
print(model, knots=TRUE)
```
   
## Estimating the model

Given the set of knots from the model object, the estimation is just a
least squares method applied to the extended set of covariates. We
want to estimate the model

\[ 
Y = \beta_0 (1-Z) + \beta_1 Z + \psi_0'U_0(X)(1-Z) + \psi_1'U_1(X)Z + u\,, 
\] 

where $U_0(X)$ and $U_1(X)$ are defined above and depends on the model
knots. The function that estimates the model is `estModel`. The
function has three arguments, but two of them are mostly used
internally by other functions. We present it in case it is needed. The
arguments are:

- `model`: A model created by the function `setModel`.

- `w0`: A list of integers to select knots for the control group from
  the model. By default, all the knots are used.
  
- `w1`: A list of integers to select knots for the treated group from
  the model. By default, all the knots are used.

We illustrate with a simple model containing only two covariates and
one knot per eligible variables.

```{r}
model <- setModel(re78~treat | ~age+married, data=nsw,
                  nbases=function(n) 2)
print(model, knots=TRUE)
fit <- estModel(model)
fit
```

The object has its own print method that returns the coefficient
estimates. A more detrailed presentation of the results can be
obtained using the `summary` method. The following is an example with
just one knot per eligible variable.

```{r}
summary(fit)
```

For example, the coefficient of `Xf0age_1` is the effect of age for
the control on `re78` when age is less than the first knot
(`r fit$model$knots0[["age"]][1]`) and `Xf0age_2` is the effect when
age is greater then the first knot. Note that the $R^2$ and adjusted
$R^2$ are different from what we obtain using the summary of the `lm`
object:

```{r}
summary(fit$lm.out)[c("r.squared","adj.r.squared")]
```

This is because our model does not contain an intercept and the $R^2$
is computed differently for models without an intercept. The
definition of the $R^2$ used by R is the following (RSS means residual
sum of squares):

\[
R^2 = 1-\frac{\mbox{RSS for the model with the regressors}}{\mbox{RSS for the model without the regressors}}
\]

In a model with an intercept, the residual of the model without the
regressors is $Y_i-\bar{Y}$, but it is equal to $Y_i$ when the model
does not have an intercept. As a result, the $R^2$ with and without an
intercept are

\[
R^2_{with} = 1-\frac{\sum_{i=1}^n \hat{e}_i^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2}
\]
\[
R^2_{without} = 1-\frac{\sum_{i=1}^n \hat{e}_i^2}{\sum_{i=1}^n Y_i^2}
\]

However, our model does contain an intercept since we include a binary
variable for both the control and treated groups. 

## The `predict` and `plot` method

The `predict` method is very similar to the `predict.lm` method. We
use the same arguments: `object`, `interval`, `se.fit`, `newdata` and
`level`. The difference is that it returns the predicted outcome for
the treated and control groups separately and the argument `vcov.`, a
function like `vcovHC` or `vcovCL` fro the `sandwich` package, can be
used to compute robust standard errors. By default, the standard
errors are computed using `vcov.lm` (valid under the homoskedasticity
assumption). Since there are many options for computing robust
standard errors, we let the user chooses what is the most appropriate
one when it is needed. The function return a list of 2 elements,
`treated` and `control`. By default (`se.fit=FALSE` and
`interval="none"`), each element contains a vector of
predictions. Here is an example with the previously fitted model
`fit`:

```{r}
predict(fit, newdata=data.frame(treat=c(1,1,0,0),age=20:23, married=1))
```

If `interval` is set to "confidence", but $`se.fit` remains equal to
`FALSE`, each element contains a matrix containing the prediction, and
the lower and upper bound of the confidence interval, with the
confidence level determined by the argument `level`. Here is an
example with the same fitted model:

```{r}
predict(fit, newdata=data.frame(treat=c(1,1,0,0),age=20:23, married=1),
        interval="confidence")
```

If `se.fit` is set to `TRUE`, each element, treated or control, is a
list with the elements `pr`, containing the predictions, and `se.fit`,
containing the standard errors. In the following, we only show the
result for the treated:

```{r}
predict(fit, newdata=data.frame(treat=c(1,1,0,0),age=20:23, married=1),
        se.fit=TRUE)$treated
```

The `predict` method is called by the `plot` method to visually assess
the predicted outcome for the treated and control groups with respect
to a given covariate, controlling for the other covariates in the
model. The arguments of the `plot` method are:

- **x**: An object of class `tlseFit`.

- **y**: An alias for `which` for compatibility with the generic
  `plot` function.
  
- **which**: covariate to plot against the outcome variable. It could
  be an integer (the position of the covariate) or a character (the
  name of the covariate)

- **interval**: The type of confidence interval to include. The
  default is "none". The other alternative is "confidence".
  
- **level**: The confidence level when `interval="confidence"`. The
  default is 0.95.
  
- **newdata**: An optional named vector of fixed values for some or
  all other covariates.
  
- **legendPos**: The position of the legend. The default is
  "topright".
  
- **vcov.**: An optional function to compute the estimated matrix of
  covariance of the least squares estimators. This argument only
  affects the confidence intervals.

- **col0, col1, lty0, lty1**: The line colors and shapes for the
  control and treated. The defaults are `col0=1` (black), `col1=2`
  (red), `lty0=1` (solid) and `lty1=2` (dashed).
  
- **add.**: Should the curves be added to an existing plot? The
  default is `FALSE`.

- **addToLegend**: An optional character string to add to the legend
  next to "treated" and "control".

- **cex**: The font size for the legend. The default is 1. 

- **ylim, xlim**: optional ranges for the y-axis and x-axis.

- **addPoints**: Should we include the scatterplot of the outcome and
  covariate to the graph? The default is `FALSE`.
  
- **FUN**: A function to determine how the other covariates are
  fixed. The default is `mean`. Note that the function is applied to
  each group separately.

- **main**: An optional title to replace the default one.

- **...*: Other arguments are passed to the `vcov.` function. 

In the following, we illustrate some examples. Consider the model:

```{r}
model1 <- setModel(re78~treat | ~age+re75+ed+married, data=nsw)
fit1 <- estModel(model1)
```

Suppose we want to compare the predicted income with respect to age or
education, holding the other covariates fixed to their group means
(the default). 

The following are two examples with some of the default arguments
modified:

```{r, fig.show='hold', out.width='50%'}
library(sandwich)
plot(fit1, "ed", col0="darkgreen", col1="darkred", lty0=2, lty1=4,
     legendPos="topleft")
plot(fit1, "age", interval='confidence', level=0.9, vcov.=vcovHC)
```

If we want to fix the other covariates using another function, we can
change the argument `FUN`. The new function must be a function of one
argument. For example, if we want to fix the other covariates to their
group medians, we set `FUN` to `median` (no quotes). We proceed the
same way for any function that requires only one argument
(e.g. `mode`). If the function requires more than one argument, we
have to create a new function. For example, if we want to fix them to
their 20\% group quantiles, we can set the argument to `function(x)
quantile(x, .20)`. The following illustrates the two cases:

```{r, fig.show='hold', out.width='50%'}
plot(fit1, "age", FUN=mode)
plot(fit1, "age", FUN=function(x) quantile(x, .20))
```

It is also possible to set some of the other covariates to a specific
value by changing the argument `newdata`. This argument must be a
named vector with the names corresponding to the variables you want to
fix. You can also add a description to the legend with the argument
`addToLegend`.

```{r, fig.show='hold', out.width='50%'}
plot(fit1, "age", newdata=c(married=1, re75=10000), addToLegend="married", cex=0.8)
plot(fit1, "age", newdata=c(married=0, re75=10000), addToLegend="non-married", cex=0.8)
```

To be better compare the two, it is also possible to have them plotted
on the same graph by setting the argument `add.` to `TRUE`. We just
need to adjust some of the arguments to better distinguish the
different curves. In the following example, we set the colors and line
shapes to different values and change the position of the legend in
the second `plot` function.

```{r, fig.align='center', out.width='70%'}
plot(fit1, "age", newdata=c(married=1, re75=10000), addToLegend="married", cex=0.8,
     ylim.=c(3000,10000))
plot(fit1, "age", newdata=c(married=0, re75=10000), addToLegend="non-married", cex=0.8,
     legendPos='topleft', col0="darkgreen", col1="darkred", lty0=4, lty1=5,
     add.=TRUE,)
```

Finally, it is also possible to add the observed points to the graph.


```{r, fig.show='hold', out.width='50%'}
plot(fit1, "ed", col0="darkgreen", col1="darkred", lty0=2, lty1=4,
     legendPos="topleft", addPoints=TRUE)
plot(fit1, "re75", addPoints=TRUE)
```

## Optimal selection of the knots

We propose two methods for selecting the knots: the backward (BTLSE) and
the forward (FTLSE) methods. For each method, we propose 3 criteria:
the asymptotic (ASY), the Akaike Information (AIC) and the Bayesian
Information (BIC). The two selection methods can be summarized as
follows:

> **BTLSE**: 
>
> 1. We estimate the model with all knots included in the model.
>
> 2. For each knot, we test if the slopes of the basis function are
>    the same before and after, and return the p-value.
>
> 3. The knots are selected using one of the following criteria
>
>    - **ASY**: We remove all knots with a p-value greater than a
>      specified threshold.
>
>     - **AIC** or **BIC**: We order the p-values in
>      descending order. Then, going from the largest to the smallest,
>      we remove the knot associated with the p-value one by one,
>      estimate the model and return the information criterion. We
>      keep the model with the smallest information citerion.


> **FTLSE**: 
>
> 1. We estimate the model by including a subset of the knots one
>    variable at the time. When we test a knot for one variable, the
>    number of knots is set to 0 for all the others. 
>
> 2. For each knot, we test if the slope of the piecewise linear
>    polynomial is the same before and after, and return the
>    p-value. The set of knots used for each test depends on the
>    following:
>
>    - Variables with 1 knot: we return the p-value of the test of
>      equality before and after the knot.
>
>    - Variables with 2 knots: we include the two knots and return the
>      p-values of the test of equality before and after for each
>      knot.
>
>    - Variables with $p$ knots ($p>2$): We test the equality before
>      and after the knot $i$, for $i=1,...,p$, using the sets of
>      knots $\{1,2\}$, $\{1,2,3\}$, $\{2,3,4\}$, ..., $\{p-2,p-1,p\}$
>      and $\{p-1,p\}$ respectively.
>
> 3. The knots are selected using one of the following criteria
>
>    - **ASY**: We remove all knots with a p-value greater than a
>      specified threshold.
>
>     - **AIC** or **BIC**: We order the p-values in ascending
>      order. Then, starting with a model with no knots and going from
>      the smallest to the highest highest p-value, we add the knot
>      associated with the p-value one by one, estimate the model and
>      return the information criterion. We keep the model with the
>      smallest information citerion.


The knot selection is done using the function `selTLSE`. The arguments are:

- **model**: An object of class `tlseModel`.

- **method**: This is the selection method. We have the choice between
  "FTLSE" (the default) and "BTLSE".

- **crit**: This is the criterion used by the selection method. We
  have the choice between "AIC" (the default), "BIC" or "ASY".
  
- **pvalT**: This is a function that returns the p-value threshold. It
  is a function of one argument, the average number of knots per
  covariate. The default is `function(p) 1/log(p)`. It is also
  possible to set it to a fix threshold. For example, `function(p)
  0.20` set the threshold to 0.2. This argument affects the result
  only when `method` is set to "ASY".
  
- **vcov.**: By default, the p-values are computed with the `lm`
  covariance matrix method `vcov`. Alternatively, we can use sandwich
  estimators like `vcovHC`. 
  
- **...**: This is used to pass arguments to the `vcov.` function.

The function returns a model of class `tlseModel` with the optimal
selection of knots. For example, we can compare the starting knots of
`model1`, with the model selected by the default arguments.

\footnotesize
```{r}
print(model1, knots=TRUE)
model2 <- selTLSE(model1)
print(model2, knots=TRUE)
```
\normalsize

For example, the method has removed all knots from `re75` for the
treated group and kept two knots for the control group. The print
method indicates which method was used to select the knots. In the
following example, we see BTLSE as selection method and BIC as
criterion. Note that the BIC selects 0 knots for all covariates.

```{r}
model3 <- selTLSE(model1, method="BTLSE", crit="BIC")
model3
```

Since the function `selTLSE` function returns a new model, we can
apply the `estModel` to it:

```{r}
estModel(selTLSE(model1, method="FTLSE", crit="BIC"))
```

## The `causalTLSE` method for `tlseFit` objects

The regression estimated by `estModel`, or the one defined in the
introduction, can be written as

\[ 
Y_i = \beta_0 (1-Z_i) + \beta_1 Z_i + \psi_0'[U_0(X_i)(1-Z_i)] + 
\psi_1'[U_1(X_i)Z_i] + u_i\mbox{ for } i=1,...,n\,.
\] 

Let $\hat\beta_0$, $\hat\beta_1$, $\hat\psi_0$ and $\hat\psi_1$ be the
least squares estimates of the above model. Then, the TLSE average
causal effect (ACE), causal effect on the treated (ACT) and causal
effect on the non-treated (ACN) are defined respectively as follows:

\begin{eqnarray*}
\mathrm{ACE} &=& \hat\beta_1-\hat\beta_0 + \hat{\phi}_1'\overline{U_1} -
\hat{\phi}_0'\overline{U_0} \\
\mathrm{ACT} &=& \hat\beta_1-\hat\beta_0 + \hat{\phi}_1'\overline{U_1Z} -
\hat{\phi}_0'\overline{U_0Z} \\
\mathrm{ACN} &=& \hat\beta_1-\hat\beta_0 + \hat{\phi}_1'\overline{U_1(1-Z)} -
\hat{\phi}_0'\overline{U_0(1-Z)} \,,
\end{eqnarray*}

where

\begin{eqnarray*}
\overline{U_j} &=& \frac{1}{n}\sum_{i=1}^n U_j(X_i) \mbox{, for j=0,1}\\
\overline{U_jZ} &=& \frac{1}{n_1}\sum_{i=1}^n U_j(X_i)Z_i \mbox{, for j=0,1}\\
\overline{U_j(1-Z)} &=& \frac{1}{n_0}\sum_{i=1}^n U_j(X_i)(1-Z_i) \mbox{, for j=0,1}
\end{eqnarray*}

and $n_0$ and $n_1$ are the sample size in the control and treated
groups. The method `causalTLSE` estimates the causal effects from
`tlseFit` objects using the knots included in the estimated model. The
arguments of the method are:

- **object**: An object of class `tlseFit`.

- **seType**: The method to compute the standard errors of the
  causality measures. By default, they are computed using an analytic
  expression derived in the paper. Alternatively, we can set the
  argument to "lm" and use the least squares standard errors based on
  the asymptotic properties.
   
- **causal**: What causality measure should the function compute? We
  have the choice between "All" (the default), "ACT", "ACE" or "ACT".
   
- **vcov.**: An alternative function used to compute the covariance
  matrix of the least squares estimates. By default, `vcov.lm` is
  used. For example, we can set the argument to `vcovHC` from the
  `sandwich` package to compute heteroskedastic robust standard
  errors.
  
- **...**: This is used to pass arguments to the `vcov.` function.

In the following example, we estimate the causal effect with the
initial knots (without selection).

```{r}
model1 <- setModel(re78 ~ treat | ~ age + re75 + ed + married, data=nsw)
fit1 <- estModel(model1)
causalTLSE(fit1)
```

We see that the selection method used to select the knots are set to
SLSE. This is explained in the section "Setting up the Model". The
method returns an object of class `causaltlse`. We see above what its
`print` method returns. The following shows its `summary` method:

```{r}
ce <- causalTLSE(fit1)
summary(ce)
```

By default, the standard errors are computed using an analytic
expression derived in the paper. In the following, we estimate the
standard errors using the HC3 type of heteroskedasticity robust
standard errors.

```{r}
ce2 <- causalTLSE(fit1, seType="lm", vcov.=vcovHC, type="HC3")
summary(ce2)
```

The object `causaltlse` inherits from the class `tlseFit`, so we can
apply the `plot` (or the `predict`) method directly on this object.

```{r, fig.align='center', out.width='65%'}
plot(ce2, "re75")
```

### The `extract` method

The package comes with an `extract` method for objects of class
`causaltlse`, which is a required method for creating Latex tables
using the `texreg` package. For example, we can compare different
methods in a single table.

```{r, eval=FALSE}
library(texreg)
c1 <- causalTLSE(fit1)
fit2 <- estModel(selTLSE(model1, method="BTLSE"))
fit3 <- estModel(selTLSE(model1, method="FTLSE"))
c2 <- causalTLSE(fit2)
c3 <- causalTLSE(fit3)
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
library(texreg)
c1 <- causalTLSE(fit1)
fit2 <- estModel(selTLSE(model1, method="BTLSE"))
fit3 <- estModel(selTLSE(model1, method="FTLSE"))
c2 <- causalTLSE(fit2)
c3 <- causalTLSE(fit3)
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```
\normalsize
\end{center}

The option `table=FALSE`, from the `texreg` package, is to remove the
Latex floating table environment. With this option, the table appears
right after the code instead of being placed somewhere else by
Latex. The arguments of the `extract` methods, which control what is
printed and can be modified through the `texreg` function, are:

- **include.nobs**: Should the number of observations be printed? The
  default is `TRUE`.
  
- **include.nknots**: Should the number of knots be printed?  The
  default is `TRUE`.

- **include.rsquared**: Should the $R^2$ be printed?  The default is
  `TRUE`.
  
- **include.adjrsquared**: Should the adjusted $R^2$ be printed?  The
  default is `TRUE`.

- **which**: Which causal effects should be printed? The options are
  "ALL" (the default), "ACE", "ACT", "ACN", "ACE-ACT", "ACE-ACN" or
  "ACT-ACN".

Here is one example on how to change some arguments:

```{r, eval=FALSE}
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE,
       which="ACE-ACT", include.adjrsquared=FALSE)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE,
       which="ACE-ACT", include.adjrsquared=FALSE)
```
\normalsize
\end{center}

## The `causalTLSE` method for `tlseModel` objects

When applied directly to `tlseModel` objects, the `causalTLSE` method
offers the possibility to select the knots and estimate the causal
effects all at once. The method also returns an object of class
`causaltlse`. The arguments are the same as the method for `tlseFit`
objects, plus the necessary arguments for the knots selection. The
following are the arguments not already defined for objects of class
`tlseFit`. The details of these arguments are presented in the section
Optimal selection of knots.

- **object**: An object of class `tlseModel`.

- **selType**: This is the selection method. We have the choice
  between "SLSE" (the default), "FTLSE" and "BTLSE". The SLSE method
  performs no selection, so all knots from the model are kept. 

- **selCrit**: This is the criterion used by the selection method. We
  have the choice between "AIC" (the default), "BIC" or "ASY".
  
- **pvalT**: This is a function that returns the p-value threshold. We
  explained this argument when we presented the `selTLSE` function.
  
For example, we can generate the previous table as follows.

```{r, eval=FALSE}
c1 <- causalTLSE(model1, selType="SLSE")
c2 <- causalTLSE(model1, selType="BTLSE")
c3 <- causalTLSE(model1, selType="FTLSE")
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
c1 <- causalTLSE(model1, selType="SLSE")
c2 <- causalTLSE(model1, selType="BTLSE")
c3 <- causalTLSE(model1, selType="FTLSE")
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```
\normalsize
\end{center}

## The `causalTLSE` method for `formula` objects

This last method, offers an alternative way of estimating the causal
effects. It allows the estimation in one step without having to first
create a model. The arguments are the same as the ones from the
`setModel` function and the `causalTLSE` method for `tlseModel'
objects. It creates the model, select the knots and estimate the
causal effects in one step. For example, we can create the previous
table as follows:

```{r, eval=FALSE}
c1 <- causalTLSE(re78 ~ treat | ~ age + re75 + ed + married, data=nsw,
                 selType="SLSE")
c2 <- causalTLSE(re78 ~ treat | ~ age + re75 + ed + married, data=nsw,
                 selType="BTLSE")
c3 <- causalTLSE(re78 ~ treat | ~ age + re75 + ed + married, data=nsw,
                 selType="FTLSE")
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
c1 <- causalTLSE(model1, selType="SLSE")
c2 <- causalTLSE(model1, selType="BTLSE")
c3 <- causalTLSE(model1, selType="FTLSE")
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```
\normalsize
\end{center}


## A simulated data set from Model 1

In the package, the data set `datSim1` is generated using the
following data generating process with a sample size of 300.

\begin{eqnarray*}
Y(0) &=& 1+X+X^2+e\\
Y(1) &=& 1-2X+u\\
Z &=& \mathrm{B}[\Lambda(1+X)]\\
Y &=& Y(1)Z + Y(0) (1-Z)\,
\end{eqnarray*}

where $X$, $e$ and $u$ are independent standard normal, $\Lambda(x)$
is the CDF of the standard logistic distribution and B(p) is the
Bernoulli distribution. The causal effects ACE, ACT and ACN are
approximately equal to -1, -1.6903 and 0.5867 (estimated using a
sample size of 10 millions). We can start by building starting model:

```{r}
data(simDat1)
mod <- setModel(Y~Z | ~X, data=simDat1)
```

Then we can compare three different methods:

```{r, eval=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="BTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="FTLSE", selCrit="BIC")
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="BTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="FTLSE", selCrit="BIC")
texreg(list(SLSE=c1, BTLSE=c2, FTLSE=c3), table=FALSE, digits=4)
```
\normalsize
\end{center}


We see that both selection methods choose to assign 0 knots for the
treated group, which is not surprising since the true $f_1(x)$ is
linear. We can compare the different fits (we ignore the FTLSE because
the selected knots are the same):

```{r, out.width='50%', fig.show='hold'}
plot(c1, "X")
curve(1-2*x, -3,3, col="darkgreen", lty=4, lwd=3, add=TRUE)
curve(1+x+x^2, -3,3, col="darkorange", lty=4, lwd=3, add=TRUE)
legend("bottomleft", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=4, lwd=3, bty='n')
plot(c2, "X")
curve(1-2*x, -3,3, col="darkgreen", lty=4, lwd=3, add=TRUE)
curve(1+x+x^2, -3,3, col="darkorange", lty=4, lwd=3, add=TRUE)
legend("bottomleft", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=4, lwd=3, bty='n')
```

We see that the piecewise polynomials are very close to the true
$f_1(x)$ and $f_2(x)$. We can see from the folllowing graph how the
lines are fit through the observations by group.

```{r, out.width='60%', fig.align='center'}
plot(c1, "X", addPoints=TRUE)
```

## A simulated data set from Model 2

The dataset `datSim2` was generated using the following data
generating process. It is change point (piecewise) regression model.

\begin{eqnarray*}
Y(0) &=& (1+X)I(X\leq-1) + (-1-X)I(X>-1) + e\\
Y(1) &=& (1-2X)I(X\leq 0) + (1+2X)I(X>0) + u\\
Z &=& \mathrm{B}[\Lambda(1+X)]\\
Y &=& Y(1)Z + Y(0) (1-Z)\,
\end{eqnarray*}

where $I(A)$ is the indicator function equal to 1 if $A$ is true, $X$,
$e$ and $u$ are independent standard normal, $\Lambda(x)$ is the CDF
of the standard logistic distribution and B(p) is the Bernoulli
distribution. The causal effects ACE, ACT and ACN are approximately
equal to 3.763, 3.858 and 3.545 (estimated with a sample size of 10
millions). We can compare the SLSE, BTLSE with AIC and BTLSE with BIC.

```{r}
data(simDat2)
mod <- setModel(Y~Z | ~X, data=simDat2)
```

```{r, eval=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="BTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="BTLSE", selCrit="AIC")
texreg(list(SLSE=c1, BTLSE.BIC=c2, BTLSE.AIC=c3), table=FALSE, digits=4)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="BTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="BTLSE", selCrit="AIC")
texreg(list(SLSE=c1, BTLSE.BIC=c2, BTLSE.AIC=c3), table=FALSE, digits=4)
```
\normalsize
\end{center}

The following shows the fit of BTLSE-AIC with the true $f_1(x)$ and
$f_0(x)$, and the observations.

```{r, out.width='50%', fig.show='hold'}
plot(c2, "X", legendPos="right", cex=.8)
curve((1-2*x)*(x<=0)+(1+2*x)*(x>0), -3,3,
      col="darkgreen", lty=3, lwd=3, add=TRUE)
curve((1+x)*(x<=-1)+(-1-x)*(x>-1),
      -3,3, col="darkorange", lty=3, lwd=3, add=TRUE)
legend("left", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=3, lwd=3, bty='n', cex=.8)
plot(c2, "X", addPoints=TRUE, legendPos="topleft", cex=.8)
```

## A simulated data set from Model 3

In the package, the data set `datSim3` is generated using the
following data generating process with a sample size of 300. This
model is presented as a case of multiple covariates.

\begin{eqnarray*}
Y(0) &=& [1+X_1+X_1^2] + [(1+X_2)I(X_2\leq-1) + (-1-X_2)I(X_2>-1)] + e\\
Y(1) &=& [1-2X_1] + [(1-2X_2)I(X_2\leq 0) + (1+2X_2)I(X_2>0)] + u\\
Z &=& \mathrm{B}[\Lambda(1+X_1+X_2)]\\
Y &=& Y(1)Z + Y(0) (1-Z)\,,
\end{eqnarray*}

where $I(A)$ is the indicator function equal to 1 if $A$ is true,
$X_1$, $X_2$, $e$ and $u$ are independent standard normal,
$\Lambda(x)$ is the CDF of the standard logistic distribution and B(p)
is the Bernoulli distribution. The causal effects ACE, ACT and ACN are
approximately equal to 2.762, 2.204 and 3.922 (estimated with a sample
size of 10 millions). We can compare the SLSE, FTLSE with AIC and
FTLSE with BIC. 

```{r}
data(simDat3)
mod <- setModel(Y~Z | ~X1+X2, data=simDat3)
```

```{r, eval=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="FTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="FTLSE", selCrit="AIC")
texreg(list(SLSE=c1, FTLSE.BIC=c2, FTLSE.AIC=c3), table=FALSE, digits=4)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="FTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="FTLSE", selCrit="AIC")
texreg(list(SLSE=c1, FTLSE.BIC=c2, FTLSE.AIC=c3), table=FALSE, digits=4)
```
\normalsize
\end{center}

To illustrate the method, since we have two covariates, we need to
plot the outcome against one covariate holding the other fixed. The
default is to fix it to its sample mean. For the true curve, we fix it
to its population mean, which is 0. We first look at the outcome
against $X_1$. By fixing $X_2$ to 0, the true curve is $X_1+X_1^2$ for
the control and $2-2X_1$ for the treated. The following graphs show how
the FTLSE-BIC method fits the curves.

```{r, out.width='50%', fig.show='hold'}
plot(c2, "X1", legendPos="right", cex=.8)
curve(x+x^2, -3,3, col="darkgreen", lty=3, lwd=3, add=TRUE)
curve(2-2*x, -3,3, col="darkorange", lty=3, lwd=3, add=TRUE)
legend("topleft", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=3, lwd=3, bty='n', cex=.8)
plot(c2, "X1", addPoints=TRUE, legendPos="topleft", cex=.8)
```

If we fix $X_1$ to 0, the true curve is $1+[(1+X_2)I(X_2\leq-1) +
(-1-X_2)I(X_2>-1)]$ for the control and $1+[(1-2X_2)I(X_2\leq 0) +
(1+2X_2)I(X_2>0)]$ for the treated. The following graphs illustrates
how these curves are approximated by FTLSE-AIC.

```{r, out.width='50%', fig.show='hold'}
plot(c2, "X2", legendPos="right", cex=.8)
curve(1+(1-2*x)*(x<=0)+(1+2*x)*(x>0), -3,3,
      col="darkgreen", lty=3, lwd=3, add=TRUE)
curve(1+(1+x)*(x<=-1)+(-1-x)*(x>-1),
      -3,3, col="darkorange", lty=3, lwd=3, add=TRUE)
legend("left", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=3, lwd=3, bty='n', cex=.8)
plot(c2, "X2", addPoints=TRUE, legendPos="topleft", cex=.8)
```

## A simulated data set from Model 4

In the package, the data set `datSim5` is generated using the
following data generating process with a sample size of 300. This
model is presented as a case of multiple covariates with
interactions..

\begin{eqnarray*}
Y(0) &=& [1+X_1+X_1^2] + [(1+X_2)I(X_2\leq-1) + (-1-X_2)I(X_2>-1)] \\
&& + [1+X_1 X_2 + (X_1X_2)^2] + e\\
Y(1) &=& [1-2X_1] + [(1-2X_2)I(X_2\leq 0) + (1+2X_2)I(X_2>0)] \\
&&+ [1-2X_1X_2] + u\\
Z &=& \mathrm{B}[\Lambda(1+X_1+X_2+X_1X_2)]\\
Y &=& Y(1)Z + Y(0) (1-Z)\,,
\end{eqnarray*}

where $I(A)$ is the indicator function equal to 1 if $A$ is true,
$X_1$, $X_2$, $e$ and $u$ are independent standard normal,
$\Lambda(x)$ is the CDF of the standard logistic distribution and B(p)
is the Bernoulli distribution. The causal effects ACE, ACT and ACN are
approximately equal to 1.763, 0.998 and 3.194 (estimated with a sample
size of 10 millions). We can compare the SLSE, FTLSE with AIC and
FTLSE with BIC. 

```{r, echo=FALSE}
sim_model <- function(n)
{
	X1 <- rnorm(n)
    X2 <- rnorm(n)
	probX <- plogis(1 + X1 + X2 + X1*X2) 
	Z <- rbinom(n, 1, probX)
	er0 <- rnorm(n)
	er1 <- rnorm(n)
	Y0 <-  (1 + X1 + X1^2) + (1 + X2)  * (X2 <= -1) + 
		(-1 - X2) * (X2 > -1) + (1+X1*X2 + X1^2*X2^2) + er0
	Y1 <- (1 - 2*X1) + (1 - 2*X2)*(X2 <= 0) + 
		(1 + 2*X2) * (X2 > 0) + (1-2*X1*X2) + er1
	Y <- Y0 * (1 - Z) + Y1 * Z   
	data.frame(Y=Y, X1=X1, X2=X2, Z=Z, Y1=Y1, Y0=Y0)
}
```

```{r}
data(simDat5)
mod <- setModel(Y~Z | ~X1*X2, data=simDat5)
```

```{r, eval=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="BTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="BTLSE", selCrit="AIC")
texreg(list(SLSE=c1, FTLSE.BIC=c2, FTLSE.AIC=c3), table=FALSE, digits=4)
```

\begin{center}
\footnotesize
```{r, results='asis', message=FALSE, echo=FALSE}
c1 <- causalTLSE(mod, selType="SLSE")
c2 <- causalTLSE(mod, selType="FTLSE", selCrit="BIC")
c3 <- causalTLSE(mod, selType="FTLSE", selCrit="AIC")
texreg(list(SLSE=c1, FTLSE.BIC=c2, FTLSE.AIC=c3), table=FALSE, digits=4)
```
\normalsize
\end{center}

In the case of multiple covariates with interactions, the shape of the
fitted outcome with respect to one covariate depends on the value of
the other covariates. Without interaction, changing the value of the
other covariates only shifts the fitted line without changing its
shape. The following graphs compare the estimated relationship between
$Y$ and $X_1$ for $X_2$ equal to its mean (left graph) and 1 (right
graph). When $X_2$ is equal to its population mean, the true curves
are $(1+x+x^2)$ for the treated and $(3-2x)$ for the control. If
$X_2=1$, the true curves become $2x+2x^2$ for the treated and $(5-4x)$
for the control.

```{r, out.width='50%', fig.show='hold'}
plot(c2, "X1", legendPos="right", cex=.8,
     main="Y VS X1 (X2 = sample mean)")
curve(1+x+x^2, -3,3, col="darkgreen", lty=3, lwd=3, add=TRUE)
curve(3-2*x, -3,3, col="darkorange", lty=3, lwd=3, add=TRUE)
legend("top", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=3, lwd=3, bty='n', cex=.8)
plot(c2, "X1", newdata=c(X2=1), legendPos="right", cex=.8,
     main="Y VS X1 (X2 = 1)")
curve(2*x+2*x^2, -3,3, col="darkgreen", lty=3, lwd=3, add=TRUE)
curve(5-4*x, -3,3, col="darkorange", lty=3, lwd=3, add=TRUE)
legend("top", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=3, lwd=3, bty='n', cex=.8)
```

The following graphs illustrate the relationship between $Y$ and $X_2$
for a given $X_1$. When $X_1$ is equal to its population mean, the
true curves are $[2+(1-2x)(x\leq 0)+(1+2x)(x>0)]$ for the treated and
$[2+(1+x)(x\leq-1)+(-1-x)(x>-1)]$ for the control. If $X_1=1$, the
true curves become $[-2x+(1-2x)(x\leq 0)+(1+2x)(x>0)]$ for the treated
and $[(4+x+x^2)+(1+x)(x\leq-1)+(-1-x)(x>-1)]$ for the control.


```{r, out.width='50%', fig.show='hold'}
plot(c2, "X2", legendPos="right", cex=.8,
     main="Y VS X2 (X1 = sample mean)")
curve(2+(1-2*x)*(x<=0)+(1+2*x)*(x>0), -3,3,
      col="darkgreen", lty=3, lwd=3, add=TRUE)
curve(2+(1+x)*(x<=-1)+(-1-x)*(x>-1),
      -3,3, col="darkorange", lty=3, lwd=3, add=TRUE)
legend("top", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=3, lwd=3, bty='n', cex=.8)
plot(c2, "X2", newdata=c(X1=1), legendPos="right", cex=.8,
     main="Y VS X2 (X1 = 1)")
curve(-2*x+(1-2*x)*(x<=0)+(1+2*x)*(x>0), -3,3,
      col="darkgreen", lty=3, lwd=3, add=TRUE)
curve(4+(1+x)*(x<=-1)+(-1-x)*(x>-1)+x+x^2,
      -3,3, col="darkorange", lty=3, lwd=3, add=TRUE)
legend("top", c("True-treated","True-control"),
       col=c("darkgreen","darkorange"), lty=3, lwd=3, bty='n', cex=.8)
```


